{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify! \n",
    "\n",
    "We are going to use word vector representations to build an Emojifier. \n",
    "\n",
    "Have you ever wanted to make your text messages more expressive? This emojifier app will help do that. So rather than writing \"Congratulations on the promotion! Lets get coffee and talk. Love you!\" the emojifier can automatically turn this into \"Congratulations on the promotion! üëç Lets get coffee and talk. ‚òïÔ∏è Love you! ‚ù§Ô∏è\"\n",
    "\n",
    "We will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (‚öæÔ∏è). In many emoji interfaces, remember that ‚ù§Ô∏è is the \"heart\" symbol rather than the \"love\" symbol. But using word vectors, we'll see that even if your training set explicitly relates only a few words to a particular emoji, our algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don't even appear in the training set. This allows us to build an accurate classifier mapping from sentences to emojis, even using a small training set. \n",
    "\n",
    "We'll start with a baseline model (Emojifier-V1) using word embeddings, then build a more sophisticated model (Emojifier-V2) that further incorporates an LSTM. \n",
    "\n",
    "Lets get started!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our dependencies\n",
    "import numpy as np\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from emoji_utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Baseline model: Emojifier-V1\n",
    "\n",
    "### 1.1 - Dataset EMOJISET\n",
    "\n",
    "Let's start by building a simple baseline classifier. \n",
    "\n",
    "We have a tiny dataset (X, Y) where:\n",
    "- X contains 132 sentences (strings)\n",
    "- Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence\n",
    "\n",
    "<img src=\"images/data_set.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 1**: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. </center></caption>\n",
    "\n",
    "- We split the dataset between training (132 examples) and testing (56 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples :  132\n",
      "Number of testing examples :  56\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = read_csv_file('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv_file('data/test_emoji.csv')\n",
    "\n",
    "print(\"Number of training examples : \",X_train.shape[0])\n",
    "print(\"Number of testing examples : \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of largest phrase   :  10\n",
      "Largest phrase in the data :  I am so impressed by your dedication to this project\n"
     ]
    }
   ],
   "source": [
    "# Here length of largest phrase will be the max length of any input sentence\n",
    "largest_phrase = max(X_train, key=len)\n",
    "maxLen = len(largest_phrase.split())\n",
    "print(\"Length of largest phrase   : \", maxLen)\n",
    "print(\"Largest phrase in the data : \", largest_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': ':baseball:', '0': '‚ù§Ô∏è', '3': ':disappointed:', '4': ':fork_and_knife:', '2': ':smile:'}\n",
      "1 : ‚öæ\n",
      "0 : ‚ù§Ô∏è\n",
      "3 : üòû\n",
      "4 : üç¥\n",
      "2 : üòÑ\n"
     ]
    }
   ],
   "source": [
    "# Emoji's we are going to use...\n",
    "print(emoji_dictionary)\n",
    "\n",
    "for key, value in emoji_dictionary.items():\n",
    "    print(key, \":\", label_to_emoji(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again üòû\n",
      "I am proud of your achievements üòÑ\n",
      "It is the worst day in my life üòû\n",
      "Miss you so much ‚ù§Ô∏è\n",
      "food is life üç¥\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(X_train[ :5], Y_train[ :5]):\n",
    "    print(i,label_to_emoji(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Overview of the Emojifier-V1\n",
    "\n",
    "We are going to implement a baseline model called \"Emojifier-v1\".  \n",
    "\n",
    "<center>\n",
    "<img src=\"images/image_1.png\" style=\"width:900px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Baseline model (Emojifier-V1).</center></caption>\n",
    "</center>\n",
    "\n",
    "The input of the model is a string corresponding to a sentence (e.g. \"I love you). In the code, the output will be a probability vector of shape (1,5), that we then pass in an argmax layer to extract the index of the most likely emoji output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 is converted to one_hot [ 0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Convert labels to one-hot vectors\n",
    "Y_train_oh = one_hot(Y_train, num_classes=5)\n",
    "Y_test_oh = one_hot(Y_test, num_classes=5)\n",
    "\n",
    "index = 127\n",
    "print(Y_train[index], 'is converted to one_hot', Y_train_oh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Implementing Emojifier-V1\n",
    "\n",
    "The first step is to convert an input sentence into the word vector representation, which then get averaged together. Similar to the previous exercise, we will use pretrained 50-dimensional GloVe embeddings. The following cell loads the `word_to_vec_map`, which contains all the vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of words in Glove_data set :  400000\n",
      " Size of each vector               :  50\n"
     ]
    }
   ],
   "source": [
    "words_to_index, index_to_words, word_to_vec_map = read_glove_vecs('glove.6B/glove.6B.50d.txt')\n",
    "\n",
    "print(\" Number of words in Glove_data set : \", len(word_to_vec_map.keys()))\n",
    "print(\" Size of each vector               : \", len(word_to_vec_map['sorry']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index of  cucumber in the vocabulary is  113317\n",
      "The word at  113317 in the vocabulary is  cucumber\n"
     ]
    }
   ],
   "source": [
    "word = 'cucumber'\n",
    "print(\"The index of \", word, \"in the vocabulary is \", words_to_index[word])\n",
    "index = words_to_index[word]\n",
    "print(\"The word at \", index, \"in the vocabulary is \", index_to_words[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(word_to_vec_map, sentence):\n",
    "    \n",
    "    # strips sentences into list of lowercase words\n",
    "    words = list(map(lambda word : word.lower(), sentence.split()))\n",
    "    #print(words)\n",
    "    \n",
    "    avg = np.zeros((50, ))\n",
    "    \n",
    "    for word in words:\n",
    "        avg += word_to_vec_map[word]\n",
    "    avg = avg/len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of avg : (50,)\n",
      "Averaged glove vector for sentence :  [  1.92513620e-01   7.97070000e-02   2.72638000e-01  -2.94728000e-01\n",
      "   6.41210000e-01   1.75848000e-01  -7.57188000e-01   1.34732240e-01\n",
      "  -2.74310200e-01  -1.23909700e-01  -8.96100000e-02   1.46049600e-01\n",
      "  -5.32302000e-01  -9.79036000e-02   8.20924000e-01   5.84444000e-02\n",
      "   9.66792000e-02   9.24422000e-02  -2.97160000e-01  -6.02572000e-01\n",
      "  -1.99194000e-01   3.33171600e-01   2.96942000e-01   5.15600000e-03\n",
      "   3.18787200e-01  -1.86422000e+00  -4.41714000e-01   1.33376000e-02\n",
      "   4.48436000e-01  -4.68292000e-01   3.57988000e+00   3.82497200e-01\n",
      "  -1.93891200e-01  -2.05173200e-01   2.20312460e-01   1.04437800e-01\n",
      "   2.71212000e-01   4.15665600e-01   2.65717600e-01  -3.47841600e-01\n",
      "  -2.02602000e-01   1.60561000e-01  -7.72248000e-02   4.19363000e-01\n",
      "  -2.07754000e-01  -1.42353600e-01  -2.44369400e-01  -5.41600000e-04\n",
      "  -6.87961200e-02   3.27289200e-01]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(word_to_vec_map, 'You had only one job')\n",
    "print(\"shape of avg :\", avg.shape)\n",
    "print(\"Averaged glove vector for sentence : \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Model\n",
    "\n",
    "We now have all the pieces to finish implementing the `model()` function. After using `sentence_to_avg()` We need to pass the average through forward propagation, compute the cost, and then backpropagate to update the softmax's parameters. \n",
    "\n",
    "Assuming here that $Yoh$ (\"Y one hot\") is the one-hot encoding of the output labels, the equations needed to implement in the forward pass and to compute the cross-entropy cost are:\n",
    "$$ z^{(i)} = W . avg^{(i)} + b$$\n",
    "$$ a^{(i)} = softmax(z^{(i)})$$\n",
    "$$ \\mathcal{L}^{(i)} = - \\sum_{k = 0}^{n_y - 1} Yoh^{(i)}_k * log(a^{(i)}_k)$$\n",
    "\n",
    "It is possible to come up with a more efficient vectorized implementation. But since we are using a for-loop to convert the sentences one at a time into the avg^{(i)} representation anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, num_iterations =400, learning_rate = 0.01):\n",
    "    \n",
    "    m = Y.shape[0]\n",
    "    \n",
    "    n_y = 5\n",
    "    n_h = 50  #Hidden layers\n",
    "    \n",
    "    # Random initialization of weights and biases\n",
    "    W = np.random.randn(n_y, n_h)/ np.sqrt(n_h)\n",
    "    b = np.random.randn(n_y, )\n",
    "    \n",
    "    Y_oh = one_hot(Y, n_y)\n",
    "    \n",
    "    for t in range(num_iterations):    #loop over number of iterations\n",
    "        for i in range(m):             #loop over number of examples\n",
    "            \n",
    "            # Forward propagation\n",
    "            avg = sentence_to_avg(word_to_vec_map, X[i])   # avg vector for each sentence\n",
    "            Z  = np.matmul(W, avg) + b                     # Dot product\n",
    "            A = softmax(Z)\n",
    "            \n",
    "            assert(A.shape == (W.shape[0], ))\n",
    "            \n",
    "            # Logistic loss\n",
    "            cost = -(np.sum(Y_oh[i] * np.log(A)))         \n",
    "            \n",
    "            # Backward propagation\n",
    "            dz = A - Y_oh[i]\n",
    "            dw = np.matmul(dz.reshape(n_y, 1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "            \n",
    "            assert(dw.shape == W.shape)\n",
    "            \n",
    "            # Updating parameters\n",
    "            W = W - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "            # Print Acuuracy\n",
    "        if (t % 100 == 0) or (t == num_iterations - 1):\n",
    "            print(\"Epoch    : \" + str(t) + \"\\nCost     : %f\"%(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "            print(\"-\"*35)\n",
    "        \n",
    "    return pred, W, b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    : 0\n",
      "Cost     : 1.747688\n",
      "Accuracy : 0.378788\n",
      "-----------------------------------\n",
      "Epoch    : 100\n",
      "Cost     : 0.076308\n",
      "Accuracy : 0.871212\n",
      "-----------------------------------\n",
      "Epoch    : 200\n",
      "Cost     : 0.046902\n",
      "Accuracy : 0.893939\n",
      "-----------------------------------\n",
      "Epoch    : 300\n",
      "Cost     : 0.037510\n",
      "Accuracy : 0.901515\n",
      "-----------------------------------\n",
      "Epoch    : 400\n",
      "Cost     : 0.032189\n",
      "Accuracy : 0.901515\n",
      "-----------------------------------\n",
      "Epoch    : 500\n",
      "Cost     : 0.028492\n",
      "Accuracy : 0.909091\n",
      "-----------------------------------\n",
      "Epoch    : 600\n",
      "Cost     : 0.025679\n",
      "Accuracy : 0.909091\n",
      "-----------------------------------\n",
      "Epoch    : 700\n",
      "Cost     : 0.023427\n",
      "Accuracy : 0.909091\n",
      "-----------------------------------\n",
      "Epoch    : 800\n",
      "Cost     : 0.021570\n",
      "Accuracy : 0.916667\n",
      "-----------------------------------\n",
      "Epoch    : 900\n",
      "Cost     : 0.020008\n",
      "Accuracy : 0.916667\n",
      "-----------------------------------\n",
      "Epoch    : 1000\n",
      "Cost     : 0.018678\n",
      "Accuracy : 0.916667\n",
      "-----------------------------------\n",
      "Epoch    : 1100\n",
      "Cost     : 0.017535\n",
      "Accuracy : 0.916667\n",
      "-----------------------------------\n",
      "Epoch    : 1199\n",
      "Cost     : 0.016555\n",
      "Accuracy : 0.916667\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "pred_train, W, b = model(X_train, Y_train, word_to_vec_map, num_iterations=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our model has pretty high accuracy on the training set. Lets now see how it does on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set \n",
      "Accuracy : 0.892857\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Set \")\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random guessing would have had 20% accuracy given that there are 5 classes. This is pretty good performance after training on only 132 examples. \n",
    "\n",
    "* In the training set, the algorithm saw the sentence \"*I love you*\" with the label ‚ù§Ô∏è. You can check however that the word \"adore\" does not appear in the training set. Nonetheless, lets see what happens if you write \"*I adore you*.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.500000\n",
      "\n",
      "predictions :  [ 1.  3.  3.  0.  3.  0.  2.  2.] \n",
      "true labels :  [ 1.  3.  2.  0.  3.  4.  3.  3.]\n",
      "\n",
      " ************** predicted emoji's for our input sentences ****************\n",
      "\n",
      "Come on lets play ‚öæ\n",
      "you are such a nasty fellow üòû\n",
      "all the negative reviews vanished üòû\n",
      "I adore you ‚ù§Ô∏è\n",
      "there were no pizzas left üòû\n",
      "Had your dinner ? ‚ù§Ô∏è\n",
      "not feeling happy üòÑ\n",
      "the movie is not good and not enjoyable üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array(['Come on lets play', 'you are such a nasty fellow', 'all the negative reviews vanished',\n",
    "                           'I adore you', 'there were no pizzas left', 'Had your dinner ?', 'not feeling happy',\n",
    "                          'the movie is not good and not enjoyable'])\n",
    "\n",
    "Y_my_labels = np.array([[1], [3], [2], [0], [3], [4], [3], [3]], dtype=np.float32)\n",
    "\n",
    "\n",
    "my_preds = predict(X_my_sentences, Y_my_labels, W, b, word_to_vec_map)\n",
    "print(\"\\npredictions : \", my_preds.flatten(), \"\\ntrue labels : \", Y_my_labels.flatten())\n",
    "print(\"\\n ************** predicted emoji's for our input sentences ****************\")\n",
    "print_predictions(X_my_sentences, my_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Amazing! Because *adore* has a similar embedding as *love*, the algorithm has generalized correctly even to a word it has never seen before. Words such as *heart*, *dear*, *beloved* or *adore* have embedding vectors similar to *love*, and so might work too.\n",
    "\n",
    "* Note though that it doesn't get `not feeling happy`, `Had your dinner?`, `all the negative reviews vanished`,\n",
    "`the movie is not good and not enjoyable` correct. This algorithm ignores word ordering, so is not good at understanding phrases like \"not happy.\" \n",
    "\n",
    "* Printing the confusion matrix can also help understand which classes are more difficult for your model. A confusion matrix shows how often an example whose label is one class (\"actual\" class) is mislabeled by the algorithm with a different class (\"predicted\" class). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ‚ù§Ô∏è    ‚öæ    üòÑ    üòû   üç¥\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "True                                   \n",
      "0            7    0    0    0    0    7\n",
      "1            0    8    0    0    0    8\n",
      "2            2    0   16    0    0   18\n",
      "3            2    0    1   13    0   16\n",
      "4            1    0    0    0    6    7\n",
      "All         12    8   17   13    6   56\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1501fdf45f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "\n",
    "\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Mislabelled Sentences\n",
    "\n",
    "* Let's look at some of the false predictions made by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   True   : work is hard\t üòû\n",
      "Predicted : work is hard\t üòÑ\n",
      "---------------------------------------\n",
      "   True   : you brighten my day\t üòÑ\n",
      "Predicted : you brighten my day\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   True   : she is a bully\t üòû\n",
      "Predicted : she is a bully\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   True   : My life is so boring\t üòû\n",
      "Predicted : My life is so boring\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   True   : will you be my valentine\t üòÑ\n",
      "Predicted : will you be my valentine\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   True   : I did not have breakfast  üç¥\n",
      "Predicted : I did not have breakfast  ‚ù§Ô∏è\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mislabelled_indices = print_mislabelled_sentences(X_test, Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations** :\n",
    "* We used a plain Network to predict emotions of our sentences.\n",
    "* Only a single layer with 50 hidden units is used and, we achieved 90% accuracy which is pretty good.\n",
    "* If you look at the errors our model made... ‚ù§Ô∏èhearts appear to be more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**What we should remember from this part**:\n",
    "- Even with a few training examples, we can get a reasonably good model for Emojifying. This is due to the generalization power word vectors gives you. \n",
    "- Emojify-V1 will perform poorly on sentences such as *\"This movie is not good and not enjoyable\"* because it doesn't understand combinations of words--it just averages all the words' embedding vectors together, without paying attention to the ordering of words. We will build a better algorithm in the next part.\n",
    "    \n",
    "<font color='green'>\n",
    "    ** Next let's implement Emojifier-V2 using LSTM Networks**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "This notebook uses the GloVe algorithm due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning.(2014)\n",
    "\n",
    "- Jeffrey Pennington, Richard Socher, and Christopher D. Manning- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- Stanford NLP (https://nlp.stanford.edu/projects/glove/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
