{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Emojifier-V2 : Using LSTMs in Keras  üòÄüòÄüòÄüòÄüòÄüòÄ\n",
    "\n",
    "   Let's build an LSTM model that takes as input word sequences. This model will be able to take word ordering into account. Emojifier-V2 will continue to use pre-trained word-embeddings to represent words, but we feed them into an LSTM, whose job is to predict the most appropriate emoji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading our dependencies \n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Activation, Dropout, LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "from emoji_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Overview of the model\n",
    "\n",
    "Here is the Emojifier-v2 we will implement:\n",
    "\n",
    "<img src=\"images/emojifier-v2.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **Figure 1**: Emojifier-V2. A 2-layer LSTM sequence classifier. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Keras and mini-batching \n",
    "\n",
    "In this exercise, we want to train Keras using mini-batches. However, most deep learning frameworks require that all sequences in the same mini-batch have the same length. This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it's just not possible to do them both at the same time.\n",
    "\n",
    "The common solution to this is to use padding. Specifically, set a maximum sequence length, and pad all sequences to the same length. For example, of the maximum sequence length is 20, we could pad every sentence with \"0\"s so that each input sentence is of length 20. Thus, a sentence \"i love you\" would be represented as $(e_{i}, e_{love}, e_{you}, \\vec{0}, \\vec{0}, \\ldots, \\vec{0})$. In this example, any sentences longer than 20 words would have to be truncated. One simple way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples :  132\n",
      "Number of testing examples :  56\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = read_csv_file('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv_file('data/test_emoji.csv')\n",
    "\n",
    "print(\"Number of training examples : \",X_train.shape[0])\n",
    "print(\"Number of testing examples : \",X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of words in Glove_data set :  400000\n",
      " Size of each GloVe vector         :  50\n"
     ]
    }
   ],
   "source": [
    "words_to_index, index_to_words, word_to_vec_map = read_glove_vecs('glove.6B/glove.6B.50d.txt')\n",
    "\n",
    "print(\" Number of words in Glove_data set : \", len(word_to_vec_map.keys()))\n",
    "print(\" Size of each GloVe vector         : \", len(word_to_vec_map['sorry']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of largest phrase   :  10\n",
      "Largest phrase in the data :  I am so impressed by your dedication to this project\n"
     ]
    }
   ],
   "source": [
    "# Here length of largest phrase will be the max length of any input sentence\n",
    "largest_phrase = max(X_train, key=len)\n",
    "max_len = len(largest_phrase.split())\n",
    "print(\"Length of largest phrase   : \", max_len)\n",
    "print(\"Largest phrase in the data : \", largest_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - The Embedding layer\n",
    "\n",
    "In Keras, the embedding matrix is represented as a \"layer\", and maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. In this part, We will learn how to create an [Embedding()](https://keras.io/layers/embeddings/) layer in Keras, initialize it with the GloVe 50-dimensional vectors loaded earlier in the notebook. Because our training set is quite small, we will not update the word embeddings but will instead leave their values fixed. But in the code below, we'll see how Keras allows us to either train or leave fixed this layer.  \n",
    "\n",
    "The `Embedding()` layer takes an integer matrix of size (batch size, max input length) as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below.\n",
    "\n",
    "<img src=\"images/embedding1.png\" style=\"width:700px;height:250px;\">\n",
    "<caption><center> **Figure **: Embedding layer. This example shows the propagation of two examples through the embedding layer. Both have been zero-padded to a length of `max_len=5`. The final dimension of the representation is  `(2,max_len,50)` because the word embeddings we are using are 50 dimensional. </center></caption>\n",
    "\n",
    "The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors).\n",
    "\n",
    "The first step is to convert all your training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # This array stores indices of words in each sentence(if len(sentence) = 3, all the other elements will be zeros)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    \n",
    "    for i in range(m):   # loop over training examples\n",
    "        \n",
    "        sentence_words = list(map(lambda word : word.lower(), X[i].split()))\n",
    "        j = 0\n",
    "        # loop over words in sentence_words\n",
    "        for word in sentence_words:\n",
    "            X_indices[i, j] = word_to_index[word]  # set X_indices[i, j] to the index of the word\n",
    "            j += 1\n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['take my money' 'deep learning' 'apple or samsung']\n",
      "X1_indices =  [[ 351935.  254258.  248489.       0.       0.]\n",
      " [ 119136.  219115.       0.       0.       0.]\n",
      " [  57797.  270970.  317225.       0.       0.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array(['take my money', 'deep learning', 'apple or samsung'])\n",
    "X1_indices = sentences_to_indices(X1, words_to_index, max_len = 5)\n",
    "\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices = \", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Let's build the `Embedding()` layer in Keras, using pre-trained word vectors. After this layer is built, we will pass the output of `sentences_to_indices()` to it as an input, and the `Embedding()` layer will return the word embeddings for a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input dim of Embedding layer : (m, maxlen)\n",
      "Output dim of Embedding layer : (m, maxlen, 50)\n"
     ]
    }
   ],
   "source": [
    "print(\" Input dim of Embedding layer : (m, maxlen)\")\n",
    "print(\"Output dim of Embedding layer : (m, maxlen, 50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, words_to_index):\n",
    "    \n",
    "    vocab_len = len(words_to_index) + 1                        # adding 1 to fit Keras embedding (requirement)\n",
    "    embedding_vec_dim = word_to_vec_map['sorry'].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    \n",
    "    embed_matrix = np.zeros((vocab_len, embedding_vec_dim))\n",
    "    \n",
    "    \n",
    "    # set each row of the embed_matrix to be the glove vector of index'th word in the vocabulary\n",
    "    for word, index in words_to_index.items():\n",
    "        embed_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "    # Keras embedding layer with the correct output/input sizes, Make sure to set trainable=False.\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=embedding_vec_dim, trainable=False)\n",
    "    \n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer.\n",
    "    embedding_layer.build((None, ))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([embed_matrix])\n",
    "    \n",
    "    return embedding_layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1] =  [-0.58402002  0.39030999  0.65281999 -0.34029999  0.19493    -0.83489001\n",
      "  0.11929    -0.57291001 -0.56844002  0.72988999 -0.56975001  0.53435999\n",
      " -0.38034001  0.22471     0.98031002 -0.29660001  0.126       0.55221999\n",
      " -0.62737    -0.082242   -0.085359    0.31514999  0.96077001  0.31986001\n",
      "  0.87878001 -1.51890004 -1.78310001  0.35639     0.96740001 -1.54970002\n",
      "  2.33500004  0.84939998 -1.23710001  1.06229997 -1.4267     -0.49056\n",
      "  0.85465002 -1.28779995  0.60203999 -0.35962999  0.28586    -0.052162\n",
      " -0.50818002 -0.63459003  0.33888999  0.28415999 -0.2034     -1.23380005\n",
      "  0.46715     0.78858   ]\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "print(\"weights[0][1] = \", embedding_layer.get_weights()[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Building the Emojifier-V2\n",
    "\n",
    "Let's now build the Emojifier-V2 model. we will do so using the embedding layer you have built, and feed its output to an LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- We'll implement `Emojify_V2()`, which builds a Keras graph of the architecture shown in Figure 1. The model takes as input an array of sentences of shape (`m`, `max_len`, ) defined by `input_shape`. It should output a softmax probability vector of shape (`m`, `C = 5`). You may need `Input(shape = ..., dtype = '...')`, [LSTM()](https://keras.io/layers/recurrent/#lstm), [Dropout()](https://keras.io/layers/core/#dropout), [Dense()](https://keras.io/layers/core/#dense), and [Activation()](https://keras.io/activations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, words_to_index):\n",
    "    \n",
    "    # sentence_indices as the input of the graph, \n",
    "    # it should be of shape input_shape and dtype 'int32' (as it contains indices)\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    \n",
    "    # embedding_layer pretrained with GloVe \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, words_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    X = LSTM(units=128, activation='tanh', kernel_initializer=glorot_uniform(seed=0), return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    X = LSTM(units=128, activation='tanh', kernel_initializer=glorot_uniform(seed=0), return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    X = Dense(5, activation='softmax')(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((max_len, ), word_to_vec_map, words_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All sentences in the dataset are less than 10 words, we chose `max_len = 10`.  If we see our architecture, it uses \"20,223,927\" parameters, of which 20,000,050 (the word embeddings) are non-trainable, and the remaining 223,877 are. Because our vocabulary size has 400,001 words (with valid indices from 0 to 400,000) there are 400,001\\*50 = 20,000,050 non-trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It's time to train our model. Emojifier-V2 `model` takes as input an array of shape (`m`, `max_len`) and outputs probability vectors of shape (`m`, `number of classes`). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, words_to_index, max_len)\n",
    "Y_train_oh = one_hot(Y_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 2s 13ms/step - loss: 0.9872 - acc: 0.9242\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9700 - acc: 0.9318\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9647 - acc: 0.9470\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9740 - acc: 0.9318\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9791 - acc: 0.9318\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9733 - acc: 0.9318\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9799 - acc: 0.9318\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9764 - acc: 0.9242\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 1.0116 - acc: 0.8939\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9740 - acc: 0.9318\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9664 - acc: 0.9318\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9505 - acc: 0.9545\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9616 - acc: 0.9394\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9677 - acc: 0.9318\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9484 - acc: 0.9621\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9709 - acc: 0.9242\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9657 - acc: 0.9394\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9699 - acc: 0.9394\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9676 - acc: 0.9394\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9502 - acc: 0.9621\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9626 - acc: 0.9394\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9702 - acc: 0.9318\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9648 - acc: 0.9470\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9502 - acc: 0.9545\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9446 - acc: 0.9621\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9430 - acc: 0.9621\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9429 - acc: 0.9621\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9424 - acc: 0.9621\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9423 - acc: 0.9621\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9419 - acc: 0.9621\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9423 - acc: 0.9621\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9420 - acc: 0.9621\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9423 - acc: 0.9621\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9441 - acc: 0.9621\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9417 - acc: 0.9621\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9513 - acc: 0.9545\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.9489 - acc: 0.947 - 0s 1ms/step - loss: 0.9445 - acc: 0.9545\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9655 - acc: 0.9394\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9524 - acc: 0.9545\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9434 - acc: 0.9621\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9422 - acc: 0.9621\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9436 - acc: 0.9621\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9427 - acc: 0.9621\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9414 - acc: 0.9621\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9409 - acc: 0.9621\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9398 - acc: 0.9697\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9528 - acc: 0.9545\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9456 - acc: 0.9621\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.9911 - acc: 0.9167\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.9491 - acc: 0.9621\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28fca56ed68>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs=110, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, words_to_index, max_len)\n",
    "Y_test_oh = one_hot(Y_test, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 5ms/step\n",
      "    test loss :  1.05882593564\n",
      "test accuracy :  0.857142857143\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "pred_test = model.predict(X_test_indices)\n",
    "print(\"    test loss : \", loss)\n",
    "print(\"test accuracy : \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Mislabelled sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Index  : 2\n",
      "   True   : he got a very nice raise\t üòÑ\n",
      "Predicted : he got a very nice raise\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   Index  : 3\n",
      "   True   : she got me a nice present\t üòÑ\n",
      "Predicted : she got me a nice present\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   Index  : 12\n",
      "   True   : This girl is messing with me\t üòû\n",
      "Predicted : This girl is messing with me\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   Index  : 21\n",
      "   True   : you brighten my day\t üòÑ\n",
      "Predicted : you brighten my day\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   Index  : 23\n",
      "   True   : she is a bully\t üòû\n",
      "Predicted : she is a bully\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   Index  : 32\n",
      "   True   : My life is so boring\t üòû\n",
      "Predicted : My life is so boring\t ‚ù§Ô∏è\n",
      "---------------------------------------\n",
      "   Index  : 46\n",
      "   True   : What you did was awesome\t üòÑ\n",
      "Predicted : What you did was awesome\t üòû\n",
      "---------------------------------------\n",
      "   Index  : 49\n",
      "   True   : go away\t üòû\n",
      "Predicted : go away\t ‚öæ\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 3, 12, 21, 23, 32, 46, 49]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_pred = np.argmax(pred_test, axis=1)\n",
    "print_mislabelled_sentences(X_test, Y_test, Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use our own sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Come on lets play ‚öæ\n",
      "you are such a nasty fellow üòû\n",
      "all the negative reviews vanished üòû\n",
      "I adore you ‚ù§Ô∏è\n",
      "there were no pizzas left üç¥\n",
      "Had your dinner ? üç¥\n",
      "not feeling happy üòû\n",
      "the movie is not good and not enjoyable üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_t = np.array(['Come on lets play', 'you are such a nasty fellow', 'all the negative reviews vanished',\n",
    "                           'I adore you', 'there were no pizzas left', 'Had your dinner ?', 'not feeling happy',\n",
    "                          'the movie is not good and not enjoyable'])\n",
    "X_t_indices = sentences_to_indices(X_t, words_to_index, max_len)\n",
    "Y_t_pred = np.argmax(model.predict(X_t_indices), axis=1)\n",
    "print_predictions(X_t, Y_t_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Previously, Emojify-V1 model did not correctly label `not feeling happy`, `there were no pizzas left`,  but our implementation of Emojiy-V2 got it right, Still `the movie is not good and not enjoyable` is incorrect. The current model still isn't very robust at understanding negation (like \"not happy\") because the training set is small and so doesn't have a lot of examples of negation. But if the training set were larger, the LSTM model would be much better than the Emojify-V1 model at understanding such complex sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color='blue'>\n",
    "**What we should remember**:\n",
    "- If we have an NLP task where the training set is small, using word embeddings can help our algorithm significantly. Word embeddings allows model to work on words in the test set that may not even have appeared in your training set. \n",
    "- Training sequence models in Keras (and in most other deep learning frameworks) requires a few important details:\n",
    "    - To use mini-batches, the sequences need to be padded so that all the examples in a mini-batch have the same length. \n",
    "    - An `Embedding()` layer can be initialized with pretrained values. These values can be either fixed or trained further on your dataset. If however the labeled dataset is small, it's usually not worth trying to train a large pre-trained set of embeddings.   \n",
    "    - `LSTM()` has a flag called `return_sequences` to decide if you would like to return every hidden states or only the last one. \n",
    "    - You can use `Dropout()` right after `LSTM()` to regularize your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üòÄüòÄüòÄüòÄüòÄüòÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
